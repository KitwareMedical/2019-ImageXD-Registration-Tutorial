{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Image Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "* Define the **registration problem**: **find the spatial transformation that aligns images in the presence of noise**\n",
    "* Understand how *insights* can be obtained from both **resampled images** and the **spatial transformation**\n",
    "* Understand how registration helps\n",
    "\n",
    "  * *Compare multiple modalities*\n",
    "  * Align multiple modalities for convolutional neural net **(CNN) segmentation**\n",
    "  * Track **changes over time**\n",
    "  * Place structures in a **common coordinate system**, e.g. an image atlas\n",
    "  * **Segment images** by registering to an **atlas**\n",
    "  * **Quantify** how structures **shrink** and **grow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "Todo: add a link to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Problem: Register a Human Lung Volume from Two Time Points\n",
    "\n",
    "Given a problem to register x-ray computed tomography (CT) images of a human lung taken at two time points, **what must be supported?**\n",
    "\n",
    "In image registration, we typically identify the two images as the **fixed** and **moving** image. Our goal is to **find the spatial transformation that makes the moving image so align with the fixed image**. Our fixed image was acquired from the patient at a baseline time point, and our moving image was acquired at a follow-up time point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, let's examine our fixed and moving images.\n",
    "\n",
    "We will use the [`itkwidgets`](https://github.com/InsightSoftwareConsortium/itkwidgets), interative Jupyter widgets to visualize images, point sets, and meshes in 2D and 3D.\n",
    "\n",
    "The `itkwidgets` are built on [The Insight Toolkit (ITK)](https://github.com/InsightSoftwareConsortium/ITK). ITK is an open-source, cross-platform toolkit for N-dimensional scientific image processing, segmentation, and registration. ITK has been collaboratively developed by a large community of research software engineers for over 20 years and serves as the foundation for many popular image registration tools such as [ANTs](https://github.com/ANTsX/ANTsPy), [elastix](http://elastix.isi.uu.nl/), or [ndreg](https://github.com/InsightSoftwareConsortium/ITKNDReg/).\n",
    "\n",
    "ITK is one of the few libraries that supports a wide variety of parametric spatial transformation models and an HDF5 file format to serialize and deserialize these models. Spatial transformation models range from rigid, similarity, affine, kernel-based spline, b-spline, to dense displacement fields and composite transformations. The HDF5 file format, important for reproducibility, transform quantification, and avoiding resampling issues related to data size, precision, and interpolation, has been adopted by projects such as [3D Slicer](https://slicer.org/) for medical imaging, [DREAM.3D](dream3d.bluequartz.net/) for material science imaging, and the [Brain Imaging Data Structure (BIDS)](https://bids.neuroimaging.io/) for neuroimaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import itk\n",
    "from itkwidgets import view, checkerboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad2ca04382d457fbf16d6652487a182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[], gradient_opacity=0.22, mode='x', point_sets=[], rendered_image=<itkImagePython.itkImageS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fixed_image = itk.imread('imgs/CT_3D_lung_fixed.mha')\n",
    "\n",
    "view(fixed_image, mode='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The CT image provides radiodensity in [Hounsefield units](https://en.wikipedia.org/wiki/Hounsfield_scale); lung tissue approaches -1000 in intensity, the value for air, and soft tissue is slightly above 0 intensity, the value for water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a5aedb20a045eeb3858f9d4e6c976b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(annotations=False, geometries=[], gradient_opacity=0.5, point_sets=[], rendered_image=<itkImagePython.i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(fixed_image, vmax=-300, gradient_opacity=0.5, ui_collapsed=True, annotations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82cc94b8ff3471fab0ec03114134d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(annotations=False, geometries=[], gradient_opacity=0.22, point_sets=[], rendered_image=<itkImagePython.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(fixed_image, vmin=-50, vmax=500, ui_collapsed=True, annotations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itkVectorD3 ([1.366, 1.366, 2.5])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b258e45b7947fb81ffcc5c91120646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(geometries=[], gradient_opacity=0.22, mode='z', point_sets=[], rendered_image=<itkImagePython.itkImageS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(fixed_image.GetSpacing())\n",
    "\n",
    "view(fixed_image, mode='z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Take-home observations**:\n",
    "\n",
    "- The resolution is anisotropic; resolution is higher in-plane versus out-of-plane.\n",
    "- The pixel type is signed short: registration requires supporting this pixel type and interpolating samples between these pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can compare the fixed and moving images by interleaving them in a checkboard pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02172e85f5374c9d8ed73fcbf8e9c20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Viewer(annotations=False, gradient_opacity=0.5, interpolation=False, rendered_image=<itkImagePy…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "moving_image = itk.imread('imgs/CT_3D_lung_moving.mha')\n",
    "\n",
    "checkerboard(moving_image, fixed_image,\n",
    "             mode='v', ui_collapsed=False, slicing_planes=True, gradient_opacity=0.5, shading=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Discussion question**: how is a checkerboard created if the images are not sampled on the same image grid?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
